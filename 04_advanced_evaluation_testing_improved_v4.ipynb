{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e340ddfd",
   "metadata": {},
   "source": [
    "# 04 — Advanced Evaluation & Testing (Aligned with 02)\n",
    "\n",
    "This notebook strengthens evaluation by **reusing the same configuration and models from `02_machine_learning.ipynb`** and adds:\n",
    "- Cross-validated metrics and robust **test-set evaluation**\n",
    "- **ROC** & **PR** curves (macro-averaged, one-vs-rest)\n",
    "- **Calibration** (reliability) curves and Brier scores\n",
    "- **Threshold tuning** for F1 (and optional custom cost ratios)\n",
    "- **Bootstrap 95% CIs** for key metrics\n",
    "- **Learning curves** (data size vs. score) — optional\n",
    "- **Error analysis**: top false positives/negatives\n",
    "- Consolidated artifacts: tables/CSVs/PNGs under `visualizations/` and `results/`\n",
    "\n",
    "> Notes\n",
    "> 1. Plots use **matplotlib** only (no seaborn), and are saved as static PNGs.\n",
    "> 2. If your data or splits are prepared in `02`, point the paths below to reuse them.\n",
    "> 3. All random seeds are fixed to **42** for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa330d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.11\n",
      "sklearn: 1.6.1\n",
      "matplotlib: 3.10.0\n"
     ]
    }
   ],
   "source": [
    "# ==== CONFIG (aligned with 02) ====\n",
    "from pathlib import Path\n",
    "import json, os, sys, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "NGRAM_RANGE = (1, 3)\n",
    "MAX_FEATURES = None      # set to an int if used in 02 (e.g., 20000)\n",
    "MIN_DF = None            # set to an int if used in 02 (e.g., 2)\n",
    "STOP_WORDS = None  # set e.g., 'english' if used in 02\n",
    "MIN_DF = 1 \n",
    "\n",
    "# Data config — EDIT these to match your project if needed\n",
    "# If 02 exported splits, point to those files instead.\n",
    "DATA_CSV = '../data/processed/misinformation_dataset.csv'          # e.g., '../data/processed/dataset.csv'\n",
    "TEXT_COL = 'text'        # change if your column name differs\n",
    "LABEL_COL = 'label'      # change if your label column differs\n",
    "\n",
    "# Optional: pre-made split files from 02 (if available)\n",
    "X_TRAIN_NPZ = None       # e.g., '../data/processed/X_train_tfidf.npz'\n",
    "X_TEST_NPZ  = None\n",
    "Y_TRAIN_NPY = None\n",
    "Y_TEST_NPY  = None\n",
    "\n",
    "# Artifact folders\n",
    "RES_DIR = Path('results'); RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIS_DIR = Path(RES_DIR / 'visualizations'); VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "import sklearn, matplotlib\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77479355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split from CSV -> train=62559  test=15640\n",
      "Train label distribution: {0: 42887, 1: 19672}\n"
     ]
    }
   ],
   "source": [
    "# ==== DATA LOADING ====\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def try_load_precomputed() -> Optional[Tuple]:\n",
    "    import numpy as np, scipy.sparse as sp\n",
    "    if X_TRAIN_NPZ and Path(X_TRAIN_NPZ).exists() and Y_TRAIN_NPY and Path(Y_TRAIN_NPY).exists():\n",
    "        from scipy.sparse import load_npz\n",
    "        X_train = load_npz(X_TRAIN_NPZ)\n",
    "        X_test  = load_npz(X_TEST_NPZ) if X_TEST_NPZ and Path(X_TEST_NPZ).exists() else None\n",
    "        y_train = np.load(Y_TRAIN_NPY)\n",
    "        y_test  = np.load(Y_TEST_NPY) if Y_TEST_NPY and Path(Y_TEST_NPY).exists() else None\n",
    "        if X_test is not None and y_test is not None:\n",
    "            return X_train, X_test, y_train, y_test, None, None\n",
    "    return None\n",
    "\n",
    "def _resolve_csv_path():\n",
    "    \"\"\"Return a valid CSV path, or raise with a helpful listing.\"\"\"\n",
    "    path = DATA_CSV\n",
    "    if path is None or not Path(path).exists():\n",
    "        candidates = sorted([p for p in Path('.').rglob('*.csv') if 'checkpoint' not in str(p).lower()])\n",
    "        if len(candidates) == 1:\n",
    "            path = str(candidates[0])\n",
    "            print(f\"Auto-detected DATA_CSV={path}\")\n",
    "        else:\n",
    "            if candidates:\n",
    "                hint = \"\\n\".join(str(p) for p in candidates[:50])\n",
    "            else:\n",
    "                hint = \"(no CSVs found)\"\n",
    "            raise FileNotFoundError(\n",
    "                \"Set DATA_CSV to a valid CSV path, or provide NPZ/NPY splits from 02.\\n\"\n",
    "                \"CSV candidates I found:\\n\" + hint\n",
    "            )\n",
    "    return path\n",
    "\n",
    "def load_or_split_from_csv() -> Tuple:\n",
    "    csv_path = _resolve_csv_path()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    assert TEXT_COL in df.columns and LABEL_COL in df.columns, f\"CSV must have columns: {TEXT_COL}, {LABEL_COL}\"\n",
    "    X = df[TEXT_COL].astype(str).values\n",
    "    y = df[LABEL_COL].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test, df, None\n",
    "\n",
    "pre = try_load_precomputed()\n",
    "if pre is None:\n",
    "    X_train, X_test, y_train, y_test, df_all, _ = load_or_split_from_csv()\n",
    "    print(f\"✓ Split from CSV -> train={len(y_train)}  test={len(y_test)}\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test, _, _ = pre\n",
    "    print(\"✓ Loaded precomputed TF-IDF splits from NPZ/NPY files\")\n",
    "\n",
    "# Show label distribution\n",
    "import numpy as np\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train label distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9678b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== VECTORIZATION (aligned with 02) ====\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_kwargs = dict(\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    dtype=np.float32\n",
    ")\n",
    "if MAX_FEATURES is not None:\n",
    "    tfidf_kwargs[\"max_features\"] = MAX_FEATURES\n",
    "if MIN_DF is not None:\n",
    "    tfidf_kwargs[\"min_df\"] = MIN_DF\n",
    "if STOP_WORDS is not None:\n",
    "    tfidf_kwargs[\"stop_words\"] = STOP_WORDS\n",
    "\n",
    "tfidf = TfidfVectorizer(**tfidf_kwargs)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF:\", X_train_tfidf.shape, \"->\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587142ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['Logistic Regression (liblinear)', 'Linear SVM', 'SGD (logistic loss)', 'Random Forest (reduced)']\n"
     ]
    }
   ],
   "source": [
    "# ==== MODELS (copied/aligned from 02) ====\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {}\n",
    "\n",
    "# 1) Logistic Regression (liblinear or saga). Use class_weight='balanced' as in 02\n",
    "models[\"Logistic Regression (liblinear)\"] = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2) Linear SVM\n",
    "models[\"Linear SVM\"] = LinearSVC(\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 3) SGD (logistic loss)\n",
    "models[\"SGD (logistic loss)\"] = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=2000,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 4) Random Forest (reduced, dense)\n",
    "USE_RF = True\n",
    "if USE_RF:\n",
    "    # reduce dimension for RF (TF-IDF is sparse/high-dim)\n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "    rf_k = min(20000, X_train_tfidf.shape[1])  # cap features for practicality\n",
    "    selector = SelectKBest(chi2, k=rf_k)\n",
    "    Xtr_rf = selector.fit_transform(X_train_tfidf, y_train).toarray().astype(np.float32)\n",
    "    Xte_rf = selector.transform(X_test_tfidf).toarray().astype(np.float32)\n",
    "    models[\"Random Forest (reduced)\"] = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "else:\n",
    "    Xtr_rf = Xte_rf = None\n",
    "\n",
    "print(\"Models:\", list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TRAIN & EVALUATE (test set) ====\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "def get_scores(y_true, scores_or_proba, labels):\n",
    "    # scores_or_proba: array-like (n_samples, n_classes) or decision scores\n",
    "    y_true_bin = label_binarize(y_true, classes=labels)\n",
    "    # Handle binary special-case\n",
    "    if y_true_bin.shape[1] == 1:\n",
    "        y_true_bin = np.hstack((1 - y_true_bin, y_true_bin))\n",
    "    # ROC-AUC (macro, OVR) with scores\n",
    "    rocauc = roc_auc_score(y_true_bin, scores_or_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "    # PR-AUC (macro): mean of per-class average_precision\n",
    "    ap_per_class = []\n",
    "    for k in range(y_true_bin.shape[1]):\n",
    "        ap_per_class.append(average_precision_score(y_true_bin[:,k], scores_or_proba[:,k]))\n",
    "    prauc = float(np.mean(ap_per_class))\n",
    "    return rocauc, prauc\n",
    "\n",
    "from collections import OrderedDict\n",
    "results = []\n",
    "\n",
    "labels_sorted = np.unique(y_test)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name.startswith(\"Random Forest\"):\n",
    "        model.fit(Xtr_rf, y_train)\n",
    "        y_pred = model.predict(Xte_rf)\n",
    "        # probabilities if available\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(Xte_rf)\n",
    "            scores = proba\n",
    "        else:\n",
    "            # build one-hot as fallback (not ideal for AUC)\n",
    "            scores = np.eye(len(labels_sorted))[y_pred]\n",
    "            proba = None\n",
    "    else:\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_test_tfidf)\n",
    "            scores = proba\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            dec = model.decision_function(X_test_tfidf)\n",
    "            # Convert decision_function to 2D (n_samples, n_classes)\n",
    "            if dec.ndim == 1:\n",
    "                dec = np.vstack([-dec, dec]).T\n",
    "            scores = dec\n",
    "            proba = None\n",
    "        else:\n",
    "            scores = np.eye(len(labels_sorted))[y_pred]\n",
    "            proba = None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    try:\n",
    "        rocauc, prauc = get_scores(y_test, scores, labels_sorted)\n",
    "    except Exception as e:\n",
    "        rocauc, prauc = None, None\n",
    "\n",
    "    results.append(OrderedDict([\n",
    "        (\"model\", name),\n",
    "        (\"accuracy\", round(acc, 4)),\n",
    "        (\"precision\", round(prec, 4)),\n",
    "        (\"recall\", round(rec, 4)),\n",
    "        (\"f1_macro\", round(f1, 4)),\n",
    "        (\"roc_auc_macro\", None if rocauc is None else round(rocauc, 4)),\n",
    "        (\"pr_auc_macro\", None if prauc is None else round(prauc, 4)),\n",
    "    ]))\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "display(res_df)\n",
    "res_df.to_csv(RES_DIR / \"model_comparison.csv\", index=False)\n",
    "print(\"✓ Saved results/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CURVES: ROC & PR (macro, per-model) ====\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_roc_pr_curves(model, name, X, y, labels, use_rf=False):\n",
    "    if use_rf:\n",
    "        X = Xte_rf\n",
    "    # Get scores/proba\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X)\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        dec = model.decision_function(X)\n",
    "        if dec.ndim == 1:\n",
    "            dec = np.vstack([-dec, dec]).T\n",
    "        scores = dec\n",
    "    else:\n",
    "        y_pred = model.predict(X)\n",
    "        scores = np.eye(len(labels))[y_pred]\n",
    "\n",
    "    y_bin = label_binarize(y, classes=labels)\n",
    "    if y_bin.shape[1] == 1:\n",
    "        y_bin = np.hstack((1 - y_bin, y_bin))\n",
    "\n",
    "    # ROC (macro)\n",
    "    fpr = dict(); tpr = dict(); roc_auc = dict()\n",
    "    for i in range(y_bin.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], scores[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(y_bin.shape[1])]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(y_bin.shape[1]):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= y_bin.shape[1]\n",
    "    roc_auc_macro = auc(all_fpr, mean_tpr)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(all_fpr, mean_tpr, label=f\"macro AUC={roc_auc_macro:.3f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"ROC — {name}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fn_roc = VIS_DIR / f\"roc_{name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(fn_roc, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Saved\", fn_roc)\n",
    "\n",
    "    # PR (macro)\n",
    "    precision = dict(); recall = dict(); ap = dict()\n",
    "    pr_grid = np.linspace(0,1,500)\n",
    "    mean_precision = np.zeros_like(pr_grid)\n",
    "    for i in range(y_bin.shape[1]):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], scores[:, i])\n",
    "        ap[i] = average_precision_score(y_bin[:, i], scores[:, i])\n",
    "        # interpolate precision over a common recall grid\n",
    "        mean_precision += np.interp(pr_grid, recall[i][::-1], precision[i][::-1], left=1.0, right=0.0)\n",
    "    mean_precision /= y_bin.shape[1]\n",
    "    ap_macro = float(np.mean(list(ap.values())))\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(pr_grid, mean_precision, label=f\"macro AP={ap_macro:.3f}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {name}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    fn_pr = VIS_DIR / f\"pr_{name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(fn_pr, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Saved\", fn_pr)\n",
    "\n",
    "# Plot for all trained models\n",
    "for name, model in models.items():\n",
    "    use_rf = name.startswith(\"Random Forest\")\n",
    "    plot_roc_pr_curves(model, name, X_test_tfidf, y_test, labels_sorted, use_rf=use_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CALIBRATION: Reliability curves & Brier score ====\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reliability_plot(probs, y_true, name):\n",
    "    # binary-only plot; for multi-class, we plot the positive class per label=1\n",
    "    y_bin = (y_true == np.unique(y_true)[-1]).astype(int)\n",
    "    prob_pos = probs[:, -1] if probs.shape[1] > 1 else probs[:, 0]\n",
    "    # create bins\n",
    "    bins = np.linspace(0.0, 1.0, 11)\n",
    "    binids = np.digitize(prob_pos, bins) - 1\n",
    "    bin_true = [y_bin[binids==i].mean() if np.any(binids==i) else np.nan for i in range(len(bins)-1)]\n",
    "    bin_pred = [(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.plot(bin_pred, bin_true, marker=\"o\")\n",
    "    plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Empirical positive rate\")\n",
    "    plt.title(f\"Reliability — {name}\")\n",
    "    fn = VIS_DIR / f\"calibration_{name.replace(' ', '_')}.png\"\n",
    "    plt.savefig(fn, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Saved\", fn)\n",
    "\n",
    "def calibrate_if_needed(model, name, Xtr, ytr, Xte):\n",
    "    # returns probabilities for test; wraps non-probabilistic models\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(Xte)\n",
    "    else:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "        (idx_tr, idx_val) = next(sss.split(Xtr, ytr))\n",
    "        if isinstance(Xtr, np.ndarray):\n",
    "            Xtr_, ytr_ = Xtr[idx_tr], ytr[idx_tr]\n",
    "        else:\n",
    "            Xtr_, ytr_ = Xtr[idx_tr], ytr[idx_tr]\n",
    "        calib = CalibratedClassifierCV(model, method=\"sigmoid\", cv=\"prefit\")\n",
    "        calib.fit(Xtr_, ytr_)\n",
    "        return calib.predict_proba(Xte)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name.startswith(\"Random Forest\"):\n",
    "        probs = calibrate_if_needed(model, name, Xtr_rf, y_train, Xte_rf)\n",
    "    else:\n",
    "        probs = calibrate_if_needed(model, name, X_train_tfidf, y_train, X_test_tfidf)\n",
    "    # Brier score (binary or multi-class one-vs-rest average)\n",
    "    if probs.shape[1] == 2:\n",
    "        y_bin = (y_test == np.unique(y_test)[-1]).astype(int)\n",
    "        bs = brier_score_loss(y_bin, probs[:,1])\n",
    "    else:\n",
    "        # macro average Brier\n",
    "        y_bin_all = label_binarize(y_test, classes=np.unique(y_test))\n",
    "        if y_bin_all.shape[1] == 1:\n",
    "            y_bin_all = np.hstack((1 - y_bin_all, y_bin_all))\n",
    "        bs = np.mean([brier_score_loss(y_bin_all[:,k], probs[:,k]) for k in range(y_bin_all.shape[1])])\n",
    "    reliability_plot(probs, y_test, name)\n",
    "    with open(RES_DIR / \"calibration_brier_scores.csv\", \"a\") as f:\n",
    "        f.write(f\"{name},{bs}\\n\")\n",
    "print(\"✓ Saved results/calibration_brier_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== THRESHOLD TUNING (optimize F1 on validation) ====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def best_threshold(y_true, scores):\n",
    "    # binary only: uses positive class column\n",
    "    y_bin = (y_true == np.unique(y_true)[-1]).astype(int)\n",
    "    s = scores[:, -1] if scores.shape[1] > 1 else scores[:, 0]\n",
    "    ts = np.linspace(0.1, 0.9, 41)\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in ts:\n",
    "        pred = (s >= t).astype(int)\n",
    "        f1 = f1_score(y_bin, pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return best_t, best_f1\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Make a small validation split\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1s = []; ts = []\n",
    "    for tr, va in skf.split(X_train_tfidf, y_train):\n",
    "        if name.startswith(\"Random Forest\"):\n",
    "            # map indices for reduced RF matrices\n",
    "            Xtr, Xva = Xtr_rf[tr], Xtr_rf[va]\n",
    "        else:\n",
    "            Xtr, Xva = X_train_tfidf[tr], X_train_tfidf[va]\n",
    "        m = model.__class__(**getattr(model, \"get_params\")())\n",
    "        m.random_state = RANDOM_STATE if hasattr(m, \"random_state\") else None\n",
    "        m.fit(Xtr, y_train[tr])\n",
    "        if hasattr(m, \"predict_proba\"):\n",
    "            sc = m.predict_proba(Xva)\n",
    "        elif hasattr(m, \"decision_function\"):\n",
    "            dec = m.decision_function(Xva)\n",
    "            if dec.ndim == 1:\n",
    "                dec = np.vstack([-dec, dec]).T\n",
    "            sc = dec\n",
    "        else:\n",
    "            # fallback not meaningful; skip\n",
    "            continue\n",
    "        t, f = best_threshold(y_train[va], sc)\n",
    "        f1s.append(f); ts.append(t)\n",
    "    if f1s:\n",
    "        t_final = float(np.median(ts))\n",
    "        with open(RES_DIR / \"thresholds.csv\", \"a\") as f:\n",
    "            f.write(f\"{name},{t_final:.3f}\\n\")\n",
    "        print(f\"{name}: tuned threshold ≈ {t_final:.3f} (median over CV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669adab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== BOOTSTRAP 95% CIs (macro-F1, ROC-AUC, PR-AUC) ====\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "\n",
    "def bootstrap_ci(metric_fn, y_true, scores, n_boot=1000, alpha=0.05):\n",
    "    vals = []\n",
    "    n = len(y_true)\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.randint(0, n, n)\n",
    "        yt = y_true[idx]\n",
    "        sc = scores[idx] if isinstance(scores, np.ndarray) else scores[idx,:]\n",
    "        vals.append(metric_fn(yt, sc))\n",
    "    vals = np.sort(vals)\n",
    "    lo = vals[int((alpha/2)*n_boot)]\n",
    "    hi = vals[int((1 - alpha/2)*n_boot)]\n",
    "    return np.mean(vals), lo, hi\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def macro_f1_from_scores(y_true, scores):\n",
    "    # derive class predictions by argmax of scores\n",
    "    y_hat = scores.argmax(axis=1)\n",
    "    # map indices back to labels\n",
    "    labels = np.unique(y_true)\n",
    "    y_hat_lab = labels[y_hat]\n",
    "    return f1_score(y_true, y_hat_lab, average=\"macro\", zero_division=0)\n",
    "\n",
    "def roc_macro_from_scores(y_true, scores):\n",
    "    ys = label_binarize(y_true, classes=np.unique(y_true))\n",
    "    if ys.shape[1] == 1:\n",
    "        ys = np.hstack((1 - ys, ys))\n",
    "    return roc_auc_score(ys, scores, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "def pr_macro_from_scores(y_true, scores):\n",
    "    ys = label_binarize(y_true, classes=np.unique(y_true))\n",
    "    if ys.shape[1] == 1:\n",
    "        ys = np.hstack((1 - ys, ys))\n",
    "    aps = []\n",
    "    for k in range(ys.shape[1]):\n",
    "        aps.append(average_precision_score(ys[:,k], scores[:,k]))\n",
    "    return float(np.mean(aps))\n",
    "\n",
    "ci_rows = []\n",
    "for name, model in models.items():\n",
    "    if name.startswith(\"Random Forest\"):\n",
    "        X = Xte_rf\n",
    "    else:\n",
    "        X = X_test_tfidf\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        sc = model.predict_proba(X)\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        dec = model.decision_function(X)\n",
    "        if dec.ndim == 1:\n",
    "            dec = np.vstack([-dec, dec]).T\n",
    "        sc = dec\n",
    "    else:\n",
    "        y_pred = model.predict(X)\n",
    "        sc = np.eye(len(np.unique(y_test)))[y_pred]\n",
    "    for metric_name, fn in [(\"f1_macro\", macro_f1_from_scores),\n",
    "                            (\"roc_auc_macro\", roc_macro_from_scores),\n",
    "                            (\"pr_auc_macro\", pr_macro_from_scores)]:\n",
    "        try:\n",
    "            mean, lo, hi = bootstrap_ci(fn, y_test, sc, n_boot=500, alpha=0.05)\n",
    "            ci_rows.append([name, metric_name, round(mean,4), round(lo,4), round(hi,4)])\n",
    "        except Exception as e:\n",
    "            ci_rows.append([name, metric_name, None, None, None])\n",
    "\n",
    "ci_df = pd.DataFrame(ci_rows, columns=[\"model\",\"metric\",\"mean\",\"ci_lo\",\"ci_hi\"])\n",
    "display(ci_df.pivot(index=\"model\", columns=\"metric\", values=[\"mean\",\"ci_lo\",\"ci_hi\"]))\n",
    "ci_df.to_csv(RES_DIR / \"bootstrap_cis.csv\", index=False)\n",
    "print(\"✓ Saved results/bootstrap_cis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840fa4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ERROR ANALYSIS: Top FP/FN ====\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def collect_errors(model, name, X, y, text_source):\n",
    "    # scores -> predictions\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        sc = model.predict_proba(X)\n",
    "        yhat = sc.argmax(axis=1)\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        dec = model.decision_function(X)\n",
    "        if dec.ndim == 1:\n",
    "            dec = np.vstack([-dec, dec]).T\n",
    "        sc = dec\n",
    "        yhat = sc.argmax(axis=1)\n",
    "    else:\n",
    "        yhat = model.predict(X)\n",
    "        sc = np.eye(len(np.unique(y)))[yhat]\n",
    "    labels = np.unique(y)\n",
    "    yhat_lab = labels[yhat]\n",
    "    errs = (yhat_lab != y)\n",
    "    if text_source is None:\n",
    "        # fall back to index as text if we don't have raw text\n",
    "        text_vals = [f\"sample_{i}\" for i in range(len(y))]\n",
    "    else:\n",
    "        text_vals = text_source\n",
    "\n",
    "    # score for predicted class\n",
    "    conf = sc[np.arange(len(y)), yhat]\n",
    "    # false positives and false negatives for positive class (last label)\n",
    "    pos_label = labels[-1]\n",
    "    idx_fp = np.where((yhat_lab == pos_label) & (y != pos_label))[0]\n",
    "    idx_fn = np.where((yhat_lab != pos_label) & (y == pos_label))[0]\n",
    "    # sort by confidence descending\n",
    "    fp_sorted = idx_fp[np.argsort(conf[idx_fp])[::-1]][:20]\n",
    "    fn_sorted = idx_fn[np.argsort(1 - conf[idx_fn])[::-1]][:20]\n",
    "\n",
    "    rows = []\n",
    "    for i in fp_sorted:\n",
    "        rows.append([\"FP\", int(i), str(y[i]), str(yhat_lab[i]), float(conf[i]), text_vals[i]])\n",
    "    for i in fn_sorted:\n",
    "        rows.append([\"FN\", int(i), str(y[i]), str(yhat_lab[i]), float(conf[i]), text_vals[i]])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"type\",\"index\",\"true\",\"pred\",\"confidence\",\"text\"])\n",
    "    out = RES_DIR / f\"errors_{name.replace(' ','_')}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"✓ Saved\", out)\n",
    "\n",
    "text_source_test = X_test if isinstance(X_test, np.ndarray) else None\n",
    "for name, model in models.items():\n",
    "    if name.startswith(\"Random Forest\"):\n",
    "        collect_errors(model, name, Xte_rf, y_test, text_source_test)\n",
    "    else:\n",
    "        collect_errors(model, name, X_test_tfidf, y_test, text_source_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92591618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== (Optional) LEARNING CURVES ====\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(estimator, name, X, y, cv=3):\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        return_times=True, scoring=\"f1_macro\", shuffle=True, random_state=RANDOM_STATE\n",
    "    )\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    test_mean = test_scores.mean(axis=1)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(train_sizes, train_mean, marker=\"o\", label=\"train\")\n",
    "    plt.plot(train_sizes, test_mean, marker=\"o\", label=\"cv\")\n",
    "    plt.xlabel(\"Training examples\"); plt.ylabel(\"F1 (macro)\")\n",
    "    plt.title(f\"Learning Curve — {name}\"); plt.legend()\n",
    "    fn = VIS_DIR / f\"learning_curve_{name.replace(' ','_')}.png\"\n",
    "    plt.savefig(fn, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Saved\", fn)\n",
    "\n",
    "# Run for light models only to keep it quick\n",
    "plot_learning_curve(models[\"Linear SVM\"], \"Linear SVM\", X_train_tfidf, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== WRITE ARTIFACT SUMMARY ====\n",
    "manifest = {\n",
    "    \"model_comparison\": str(RES_DIR / \"model_comparison.csv\"),\n",
    "    \"bootstrap_cis\": str(RES_DIR / \"bootstrap_cis.csv\"),\n",
    "    \"brier_scores\": str(RES_DIR / \"calibration_brier_scores.csv\"),\n",
    "    \"thresholds\": str(RES_DIR / \"thresholds.csv\"),\n",
    "    \"error_lists\": [str(p) for p in sorted(RES_DIR.glob(\"errors_*.csv\"))],\n",
    "    \"visualizations\": [str(p) for p in sorted(VIS_DIR.glob(\"*.png\"))]\n",
    "}\n",
    "with open(RES_DIR / \"README.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"✓ Wrote results/README.json\")\n",
    "pd.DataFrame({k:[v] if not isinstance(v, list) else [\", \".join(v)] for k,v in manifest.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
